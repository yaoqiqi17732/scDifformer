{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb08bc7c-fd5a-405b-825c-63e4d299adc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23698\n"
     ]
    }
   ],
   "source": [
    "from geneformer import EmbExtractor\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import geneformer\n",
    "with open(geneformer.tokenizer.TOKEN_DICTIONARY_FILE, 'rb') as fp:\n",
    "    a = pickle.load(fp)\n",
    "print(len(a))\n",
    "from datasets import load_from_disk\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import scanpy as sc\n",
    "from geneformer.in_silico_perturber import downsample_and_sort, \\\n",
    "                                 gen_attention_mask, \\\n",
    "                                 get_model_input_size, \\\n",
    "                                 load_and_filter, \\\n",
    "                                 load_model, \\\n",
    "                                 pad_tensor_list, \\\n",
    "                                 quant_layers, \\\n",
    "                                 pad_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59fbf7c-650a-4c81-a2f7-a296f6730fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310c80ef-f205-4258-9e96-ec1785f7cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176a299-2cee-4cf0-b646-6dc2416fe168",
   "metadata": {},
   "source": [
    "## 对simulate数据进行tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49fab335-2dae-451c-b6ba-8bd821297ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_number = '_lung2'\n",
    "#pseudo_adata = sc.read_h5ad('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset' + str(simulation_number) + '/embedding_diffusion/singlecell/pseudo/pseudo_adata.h5ad')\n",
    "ST_adata = sc.read_h5ad('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset' + str(simulation_number) + '/embedding_diffusion/ST/ST_adata.h5ad')\n",
    "sc_adata = sc.read_h5ad('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset' + str(simulation_number) + '/embedding_diffusion/sc/sc_adata.h5ad')\n",
    "ST_adata.obs = ST_adata.obs.rename(columns={'Proliferating_NK/T':'Proliferating_NK_T'})\n",
    "\n",
    "ST_adata.var['ensembl_id'] = list(ST_adata.var_names)\n",
    "sc_adata.var['ensembl_id'] = list(sc_adata.var_names)\n",
    "ST_adata.obs['n_counts'] = np.sum(ST_adata.X, axis=1).astype(int)\n",
    "sc_adata.obs['n_counts'] = np.sum(sc_adata.X, axis=1).astype(int)\n",
    "\n",
    "#保存成loom文件\n",
    "#pseudo_adata.write_loom('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/singlecell/pseudo/pseudo_adata.loom', write_obsm_varm=True)\n",
    "ST_adata.write_loom('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/ST/ST_adata.loom', write_obsm_varm=True)\n",
    "sc_adata.write_loom('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/sc/sc_adata.loom', write_obsm_varm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61db9320-2168-409d-b652-525c08cc46e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54767/800111254.py:22: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.\n",
      "  ST_adata.var['ensembl_id'] = ensembl_id\n",
      "/tmp/ipykernel_54767/800111254.py:23: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.\n",
      "  sc_adata.var['ensembl_id'] = ensembl_id\n"
     ]
    }
   ],
   "source": [
    "simulation_number = 12\n",
    "#pseudo_adata = sc.read_h5ad('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset' + str(simulation_number) + '/embedding_diffusion/singlecell/pseudo/pseudo_adata.h5ad')\n",
    "ST_adata = sc.read_h5ad('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset' + str(simulation_number) + '/embedding_diffusion/ST/ST_adata.h5ad')\n",
    "sc_adata = sc.read_h5ad('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset' + str(simulation_number) + '/embedding_diffusion/sc/sc_adata.h5ad')\n",
    "\n",
    "#数据预处理\n",
    "name_id = pd.read_csv('/mnt/nfs/wbzhang/data/celltype_annotation/gene_info_table.csv')\n",
    "name_id = name_id[(name_id.gene_type == 'protein_coding') | (name_id.gene_type == 'miRNA')]\n",
    "\n",
    "common_genes = list(set(name_id['gene_name']).intersection(set(ST_adata.var_names)))\n",
    "print(len(common_genes))\n",
    "#pseudo_adata = pseudo_adata[:, common_genes]\n",
    "ST_adata = ST_adata[:, common_genes]\n",
    "sc_adata = sc_adata[:, common_genes]\n",
    "\n",
    "# mapping the enseml_id\n",
    "ensembl_id = []\n",
    "for gene in ST_adata.var_names:\n",
    "    id = name_id[name_id.gene_name == gene].ensembl_id.values[0]\n",
    "    ensembl_id.append(id)\n",
    "#pseudo_adata.var['ensembl_id'] = ensembl_id\n",
    "ST_adata.var['ensembl_id'] = ensembl_id\n",
    "sc_adata.var['ensembl_id'] = ensembl_id\n",
    "\n",
    "# n_counts 一定要算，cell_type和organ可以没有\n",
    "#pseudo_adata.obs['n_counts'] = np.sum(pseudo_adata.X, axis=1).astype(int)\n",
    "ST_adata.obs['n_counts'] = np.sum(ST_adata.X, axis=1).astype(int)\n",
    "sc_adata.obs['n_counts'] = np.sum(sc_adata.X, axis=1).astype(int)\n",
    "\n",
    "#保存成loom文件\n",
    "#pseudo_adata.write_loom('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/singlecell/pseudo/pseudo_adata.loom', write_obsm_varm=True)\n",
    "ST_adata.write_loom('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/ST/ST_adata.loom', write_obsm_varm=True)\n",
    "sc_adata.write_loom('/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/sc/sc_adata.loom', write_obsm_varm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f48dc2d-7a31-4297-a5cc-74d68689e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing /mnt/nfs/wbzhang/stdgcn/data/simulate/dataset_lung2/embedding_diffusion/sc/sc_adata.loom\n",
      "/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset_lung2/embedding_diffusion/sc/sc_adata.loom has no column attribute 'filter_pass'; tokenizing all cells.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd0de10f8f6498f9e87b4d65e89d696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a85b6447a0a4a0d9129bc4d79c9717e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec485b1655b34d2ca449f7d894f87851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing /mnt/nfs/wbzhang/stdgcn/data/simulate/dataset_lung2/embedding_diffusion/ST/ST_adata.loom\n",
      "/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset_lung2/embedding_diffusion/ST/ST_adata.loom has no column attribute 'filter_pass'; tokenizing all cells.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e030ef75f749ed95a5fedb03fa483f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c5e704dc454560a4207fa312b33a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e15180c5da4bcb802cdf892887e33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始tokenize# 开始tokenize\n",
    "from geneformer import TranscriptomeTokenizer\n",
    "data_path = '/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/'\n",
    "\n",
    "for t in ['sc', 'ST']:\n",
    "    loom_data_directory = data_path + t + '/'\n",
    "    output_directory = loom_data_directory\n",
    "    output_prefix = 'tokenized_adata'\n",
    "    tk = TranscriptomeTokenizer(nproc=16)\n",
    "    tk.tokenize_data(loom_data_directory, output_directory, output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c682d-ad0c-4eaa-a422-09b3d7d49e6a",
   "metadata": {},
   "source": [
    "## Raw code for geneformer embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572245f2-6f64-4c1f-b068-5f09cb12776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def WB_padding(input_data, length_final, pad_token_id):\n",
    "    data = input_data['input_ids']\n",
    "    new_input_ids = []\n",
    "    lengths = []\n",
    "    for cell in data:\n",
    "        if len(cell) < length_final:\n",
    "            cell_tensor = torch.tensor(cell)\n",
    "            cell_tensor = pad_tensor(cell_tensor, pad_token_id, length_final)\n",
    "            new_input_ids.append(list(cell_tensor.numpy()))\n",
    "        else:\n",
    "            new_input_ids.append(cell[:length_final])\n",
    "        lengths.append(length_final)\n",
    "    dataset_dict = {\"input_ids\": new_input_ids, 'length': lengths}\n",
    "    output_dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "    return output_dataset\n",
    "#WB_padding(filtered_input_data, length_final=1000, pad_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3c743d-d61a-4202-b342-f2a9b4ddee8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /mnt/nfs/xz/geneformer/Geneformer34m/examples/pretraining_new_model/output_own/models/230813_170132_geneformer_30M_L12_emb512_SL2048_E3_B12_LR0.0005_LSlinear_WU10000_Oadamw_DS64/models and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'bert.pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 11\n",
      "model_input_size: 2048\n",
      "forward_batch_size: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9baa0bcbc64fcb8977261b421e36c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simulation_number = '_lung2'\n",
    "target = 'sc'\n",
    "\n",
    "from geneformer.tokenizer import TOKEN_DICTIONARY_FILE\n",
    "from tqdm.notebook import trange\n",
    "import torch\n",
    "import os\n",
    "model_type=\"diffusion_12L\"\n",
    "num_classes=0\n",
    "emb_mode=\"cell\"\n",
    "cell_emb_style = None #利用 mean 的方式，后面需要改进\n",
    "filter_data=None\n",
    "max_ncells=30000\n",
    "emb_layer=-1\n",
    "emb_label=None\n",
    "labels_to_plot=None\n",
    "forward_batch_size=100\n",
    "nproc=128\n",
    "token_dictionary_file=TOKEN_DICTIONARY_FILE\n",
    "\n",
    "with open(token_dictionary_file, \"rb\") as f:\n",
    "    gene_token_dict = pickle.load(f)\n",
    "pad_token_id = gene_token_dict.get(\"<pad>\")\n",
    "\n",
    "pretrain_model_path = '/mnt/nfs/xz/diffusion/db/Diffusion-BERT/model_out/230901_025119_model_name_gene_lr_5e-06_seed_42_numsteps_128_sample_Categorical_schedule_mutual_hybridlambda_0.01_wordfreqlambda_0.3_fromscratch_False_timestep_none_ckpts/best_0_14999.th'\n",
    "#pretrain_model_path = '/mnt/nfs/xz/diffusion/db/Diffusion-BERT/model_out/230909_225342_model_name_gene_lr_1e-08_seed_42_numsteps_2048_sample_Categorical_schedule_mutual_hybridlambda_0.01_wordfreqlambda_0.3_fromscratch_False_timestep_none_ckpts/best_0_237404.th'\n",
    "data_path = '/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/'+target+'/tokenized_adata.dataset'\n",
    "output_directory = '/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/embed_output'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# extrac_embs\n",
    "model_directory = pretrain_model_path\n",
    "input_data_file = data_path\n",
    "output_directory = output_directory\n",
    "output_prefix = \"emb_results\"\n",
    "\n",
    "filtered_input_data = load_and_filter(filter_data, nproc, input_data_file)\n",
    "downsampled_data = downsample_and_sort(filtered_input_data, max_ncells) #取出max cell 的数据\n",
    "\n",
    "# 修改： 对downsample_data数据进行 2048 padding处理\n",
    "WB_input_data = WB_padding(filtered_input_data, length_final=2048, pad_token_id=pad_token_id)\n",
    "\n",
    "model = load_model(model_type, num_classes, model_directory)\n",
    "layer_to_quant = quant_layers(model)+ emb_layer \n",
    "print('layer:', layer_to_quant) # embedding 的层数\n",
    "\n",
    "#get_embs\n",
    "model_input_size = get_model_input_size(model)\n",
    "print('model_input_size:', model_input_size)\n",
    "total_batch_length = len(filtered_input_data)\n",
    "if ((total_batch_length-1)/forward_batch_size).is_integer():\n",
    "    forward_batch_size = forward_batch_size-1\n",
    "print('forward_batch_size:', forward_batch_size)\n",
    "#max_len = max(filtered_input_data['length'])\n",
    "#start embedding\n",
    "#embs_total = torch.zeros([len(WB_input_data), 512*128]).to('cuda')\n",
    "embs_total = torch.zeros([len(WB_input_data), 2048*512]).to('cuda')\n",
    "for i in trange(0, total_batch_length, forward_batch_size):\n",
    "    max_range = min(i+forward_batch_size, total_batch_length)\n",
    "    # 数据修改在这里\n",
    "    minibatch = WB_input_data.select([i for i in range(i, max_range)])\n",
    "    max_len = max(minibatch[\"length\"]) # 取最长的句子长度，作为后续padding的标准\n",
    "    original_lens = torch.tensor(minibatch[\"length\"]).to(\"cuda\")\n",
    "    minibatch.set_format(type=\"torch\")\n",
    "    input_data_minibatch = minibatch[\"input_ids\"]\n",
    "    input_data_minibatch = pad_tensor_list(input_data_minibatch, \n",
    "                                       max_len, \n",
    "                                       pad_token_id, \n",
    "                                       model_input_size)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids = input_data_minibatch.to(\"cuda\"),\n",
    "            attention_mask = gen_attention_mask(minibatch)\n",
    "        )\n",
    "    embs_i = outputs.hidden_states[layer_to_quant]\n",
    "    if cell_emb_style == 'maxpooling':\n",
    "        maxpool = torch.nn.MaxPool2d(4, stride=4, padding=1)\n",
    "        embed_pooling = maxpool(embs_i)\n",
    "        embs_total[i:(i+forward_batch_size)] = embed_pooling.flatten(1)\n",
    "    elif cell_emb_style == 'avgpooling':\n",
    "        avgpool = torch.nn.AvgPool2d(4, stride=4, padding=1)\n",
    "        embed_pooling = avgpool(embs_i)\n",
    "        embs_total[i:(i+forward_batch_size)] = embed_pooling.flatten(1)\n",
    "    else:\n",
    "        embed_pooling = None\n",
    "        embs_total[i:(i+forward_batch_size)] = embs_i.flatten(1)\n",
    "    del outputs\n",
    "    del minibatch\n",
    "    del input_data_minibatch\n",
    "    del embs_i\n",
    "    del embed_pooling\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc96296-e502-4bbe-b00b-49ae940c67d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1048576])\n"
     ]
    }
   ],
   "source": [
    "ST_embed = embs_total.cpu()\n",
    "print(ST_embed.shape)\n",
    "del embs_total\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e77dabc-526c-4f42-86a7-3c71de764ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1048576])\n"
     ]
    }
   ],
   "source": [
    "sc_embed = embs_total.cpu()\n",
    "print(sc_embed.shape)\n",
    "del embs_total\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98745d7-f550-4216-b031-abc7b8f35e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1048576])\n",
      "torch.Size([10000, 1048576])\n"
     ]
    }
   ],
   "source": [
    "print(ST_embed.shape)\n",
    "print(sc_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04cc3507-7a62-4101-9511-b6db16503f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一起PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = torch.cat((ST_embed, sc_embed), dim=0).numpy()\n",
    "pca=PCA(n_components=1024)\n",
    "pca_results=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "672f7e6d-efd9-4a74-865b-8d60c67be47c",
   "metadata": {},
   "source": [
    "# 分开PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = ST_embed.numpy()\n",
    "pca = PCA(n_components=512)\n",
    "pca_results=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0263cbd7-7af4-43b9-a39f-97285f3b3807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb3bc828-2fae-4c78-844d-37669f13c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "embeds = torch.tensor(pca_results, dtype=torch.float64)\n",
    "torch.save(embeds, '/mnt/nfs/wbzhang/stdgcn/data/simulate/dataset'+str(simulation_number)+'/embedding_diffusion/12L_scST_total_pca1024_embed.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEW_geneformer",
   "language": "python",
   "name": "new_geneformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
